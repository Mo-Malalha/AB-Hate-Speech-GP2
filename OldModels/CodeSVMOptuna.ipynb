{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tkJ7UT3zam-T",
    "outputId": "c9741776-246f-4c94-8436-2ae0ece999e4",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn-intelex in c:\\users\\win 10\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (2024.1.0)\n",
      "Requirement already satisfied: daal4py==2024.1.0 in c:\\users\\win 10\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from scikit-learn-intelex) (2024.1.0)\n",
      "Requirement already satisfied: scikit-learn>=0.22 in c:\\users\\win 10\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from scikit-learn-intelex) (1.4.1.post1)\n",
      "Requirement already satisfied: daal==2024.1.0 in c:\\users\\win 10\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from daal4py==2024.1.0->scikit-learn-intelex) (2024.1.0)\n",
      "Requirement already satisfied: numpy>=1.19 in c:\\users\\win 10\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from daal4py==2024.1.0->scikit-learn-intelex) (1.24.3)\n",
      "Requirement already satisfied: tbb==2021.* in c:\\users\\win 10\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from daal==2024.1.0->daal4py==2024.1.0->scikit-learn-intelex) (2021.11.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\win 10\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from scikit-learn>=0.22->scikit-learn-intelex) (1.12.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\win 10\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from scikit-learn>=0.22->scikit-learn-intelex) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\win 10\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from scikit-learn>=0.22->scikit-learn-intelex) (3.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-learn-intelex\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "1aefcjXug0Vx"
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'TypeAliasType' from 'typing_extensions' (C:\\Users\\win 10\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\typing_extensions.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 25\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m\n\u001b[0;32m     24\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmatplotlib\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minline\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 25\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01moptuna\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msvm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SVC\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearnex\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msvm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SVC \u001b[38;5;28;01mas\u001b[39;00m SVC_Intel\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\optuna\\__init__.py:5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01moptuna\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m integration\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01moptuna\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m logging\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01moptuna\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m multi_objective\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01moptuna\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pruners\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01moptuna\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m samplers\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\optuna\\multi_objective\\__init__.py:2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01moptuna\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_imports\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _LazyImport\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01moptuna\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmulti_objective\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m samplers\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01moptuna\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmulti_objective\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m study\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01moptuna\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmulti_objective\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m trial\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\optuna\\multi_objective\\samplers\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01moptuna\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmulti_objective\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msamplers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_adapter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _MultiObjectiveSamplerAdapter\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01moptuna\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmulti_objective\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msamplers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_base\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseMultiObjectiveSampler\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01moptuna\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmulti_objective\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msamplers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_motpe\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MOTPEMultiObjectiveSampler\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\optuna\\multi_objective\\samplers\\_adapter.py:6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01moptuna\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m multi_objective\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01moptuna\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseDistribution\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01moptuna\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msamplers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseSampler\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01moptuna\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstudy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Study\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01moptuna\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrial\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FrozenTrial\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\optuna\\samplers\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01moptuna\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msamplers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nsgaii\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01moptuna\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msamplers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_base\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseSampler\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01moptuna\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msamplers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_brute_force\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BruteForceSampler\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\optuna\\samplers\\nsgaii\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01moptuna\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msamplers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnsgaii\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_crossovers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_base\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseCrossover\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01moptuna\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msamplers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnsgaii\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_crossovers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_blxalpha\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BLXAlphaCrossover\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01moptuna\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msamplers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnsgaii\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_crossovers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_sbx\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SBXCrossover\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\optuna\\samplers\\nsgaii\\_crossovers\\_base.py:5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mabc\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01moptuna\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstudy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Study\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mBaseCrossover\u001b[39;00m(abc\u001b[38;5;241m.\u001b[39mABC):\n\u001b[0;32m      9\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Base class for crossovers.\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \n\u001b[0;32m     11\u001b[0m \u001b[38;5;124;03m    A crossover operation is used by :class:`~optuna.samplers.NSGAIISampler`\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;124;03m        parameters (uniform crossover) is built-in into :class:`~optuna.samplers.NSGAIISampler`.\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\optuna\\study\\__init__.py:4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01moptuna\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstudy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_study_direction\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StudyDirection\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01moptuna\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstudy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_study_summary\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StudySummary\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01moptuna\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstudy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstudy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m copy_study\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01moptuna\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstudy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstudy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m create_study\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01moptuna\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstudy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstudy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m delete_study\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\optuna\\study\\study.py:23\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01moptuna\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pruners\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01moptuna\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m samplers\n\u001b[1;32m---> 23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01moptuna\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m storages\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01moptuna\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m trial \u001b[38;5;28;01mas\u001b[39;00m trial_module\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01moptuna\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_convert_positional_args\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m convert_positional_args\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\optuna\\storages\\__init__.py:5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01moptuna\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_callbacks\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RetryFailedTrialCallback\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01moptuna\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstorages\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_base\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseStorage\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01moptuna\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstorages\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_cached_storage\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _CachedStorage\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01moptuna\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstorages\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_heartbeat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m fail_stale_trials\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01moptuna\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstorages\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_in_memory\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m InMemoryStorage\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\optuna\\storages\\_cached_storage.py:19\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01moptuna\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstorages\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseStorage\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01moptuna\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstorages\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_heartbeat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseHeartbeat\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01moptuna\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstorages\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_rdb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstorage\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RDBStorage\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01moptuna\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstudy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_frozen\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FrozenStudy\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01moptuna\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstudy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_study_direction\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StudyDirection\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\optuna\\storages\\_rdb\\storage.py:31\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01moptuna\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstorages\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_base\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DEFAULT_STUDY_NAME_PREFIX\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01moptuna\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstorages\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_heartbeat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseHeartbeat\n\u001b[1;32m---> 31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01moptuna\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstorages\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_rdb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TrialValueModel\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01moptuna\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstudy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_frozen\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FrozenStudy\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01moptuna\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstudy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_study_direction\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StudyDirection\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\optuna\\storages\\_rdb\\models.py:8\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Optional\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Tuple\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msqlalchemy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m asc\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msqlalchemy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m case\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msqlalchemy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CheckConstraint\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sqlalchemy\\__init__.py:12\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m__future__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m annotations\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Any\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m util \u001b[38;5;28;01mas\u001b[39;00m _util\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AdaptedConnection \u001b[38;5;28;01mas\u001b[39;00m AdaptedConnection\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseRow \u001b[38;5;28;01mas\u001b[39;00m BaseRow\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sqlalchemy\\util\\__init__.py:15\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m preloaded \u001b[38;5;28;01mas\u001b[39;00m preloaded\n\u001b[1;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_collections\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m coerce_generator_arg \u001b[38;5;28;01mas\u001b[39;00m coerce_generator_arg\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_collections\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m coerce_to_immutabledict \u001b[38;5;28;01mas\u001b[39;00m coerce_to_immutabledict\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_collections\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m column_dict \u001b[38;5;28;01mas\u001b[39;00m column_dict\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sqlalchemy\\util\\_collections.py:38\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mweakref\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_has_cy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HAS_CYEXTENSION\n\u001b[1;32m---> 38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_non_string_iterable\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Literal\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Protocol\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sqlalchemy\\util\\typing.py:56\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping_extensions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TypeGuard \u001b[38;5;28;01mas\u001b[39;00m TypeGuard  \u001b[38;5;66;03m# 3.10\u001b[39;00m\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping_extensions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Self \u001b[38;5;28;01mas\u001b[39;00m Self  \u001b[38;5;66;03m# 3.11\u001b[39;00m\n\u001b[1;32m---> 56\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping_extensions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TypeAliasType \u001b[38;5;28;01mas\u001b[39;00m TypeAliasType  \u001b[38;5;66;03m# 3.12\u001b[39;00m\n\u001b[0;32m     58\u001b[0m _T \u001b[38;5;241m=\u001b[39m TypeVar(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_T\u001b[39m\u001b[38;5;124m\"\u001b[39m, bound\u001b[38;5;241m=\u001b[39mAny)\n\u001b[0;32m     59\u001b[0m _KT \u001b[38;5;241m=\u001b[39m TypeVar(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_KT\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'TypeAliasType' from 'typing_extensions' (C:\\Users\\win 10\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\typing_extensions.py)"
     ]
    }
   ],
   "source": [
    "# !pip install vaderSentiment\n",
    "# !pip install textstat\n",
    "# !pip install pickle\n",
    "# !pip install joblib\n",
    "# !pip install optuna\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import joblib\n",
    "import sys\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "from nltk.stem.porter import *\n",
    "import string\n",
    "import re\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer as VS\n",
    "from textstat.textstat import *\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import LinearSVC\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "%matplotlib inline\n",
    "import optuna\n",
    "from sklearn.svm import SVC\n",
    "from sklearnex.svm import SVC as SVC_Intel\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uUFTkj8RmtIf",
    "outputId": "76ebdb87-af6a-4a5d-f7d0-8d6d663cf299"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearnex'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearnex\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m patch_sklearn\n\u001b[0;32m      2\u001b[0m patch_sklearn()\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearnex'"
     ]
    }
   ],
   "source": [
    "from sklearnex import patch_sklearn\n",
    "patch_sklearn()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vx0-JSQTg0V3"
   },
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YnbGy2WHg0V4"
   },
   "outputs": [],
   "source": [
    "# df = pd.read_csv(\"E:\\lenovo\\Documents\\Jupyterstuff\\gp2\\labeled_data.csv\") Put own path or use colab under\n",
    "df = pd.read_csv(\"labeled_data (1).csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "8JZAdiaUg0V4",
    "outputId": "433eb156-a038-4bf5-9449-1d9235dca525"
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "rH8R67Hdg0V6",
    "outputId": "2cccaba2-b227-4761-84e5-539f373350fd"
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "clNjQLbug0V7",
    "outputId": "9787eec7-5035-49b8-dea2-560a74d2f326"
   },
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bmg4LFE7g0V7"
   },
   "source": [
    "### Columns key:\n",
    "count = number of CrowdFlower users who coded each tweet (min is 3, sometimes more users coded a tweet when judgments were determined to be unreliable by CF).\n",
    "\n",
    "\n",
    "hate_speech = number of CF users who judged the tweet to be hate speech.\n",
    "\n",
    "\n",
    "offensive_language = number of CF users who judged the tweet to be offensive.\n",
    "\n",
    "\n",
    "neither = number of CF users who judged the tweet to be neither offensive nor non-offensive.\n",
    "\n",
    "\n",
    "class = class label for majority of CF users.\n",
    "\n",
    "    0 - hate speech\n",
    "    1 - offensive  language\n",
    "    2 - neither\n",
    "\n",
    "tweet = raw tweet text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 450
    },
    "id": "NhJ8DX0Vg0V8",
    "outputId": "33189ee5-4e32-4fc7-f31b-29dead8d75f6"
   },
   "outputs": [],
   "source": [
    "df['class'].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SMbRITcPg0V8"
   },
   "source": [
    "This histogram shows the imbalanced nature of the task - most tweets containing \"hate\" words as defined by Hatebase were\n",
    "only considered to be offensive by the CF coders. More tweets were considered to be neither hate speech nor offensive language than were considered hate speech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ca2M7ZIrg0V9"
   },
   "outputs": [],
   "source": [
    "tweets=df.tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pHo_u5_-rGTS"
   },
   "outputs": [],
   "source": [
    "# This section consisted of the basic stuff, reading the file, using describe on the data to find out the \"statistics\" of each column\n",
    "# Then we declared the 3 classes we will work on, 0 for hate, 1 for offensive, 2 for neither.\n",
    "# This histogram shows the imbalanced nature of the task,\n",
    "# most tweets containing \"hate\" words as defined by Hatebase were only considered to be offensive by the CF coders.\n",
    "# More tweets were considered to be neither hate speech nor offensive language than were considered hate speech.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1GlL1lz6g0V-"
   },
   "source": [
    "## Feature generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CoeBKNZkg0V-",
    "outputId": "6fc52216-6ce1-467b-e99d-807958d01666"
   },
   "outputs": [],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nGUnBqm0g0V_"
   },
   "outputs": [],
   "source": [
    "stopwords=stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "\n",
    "other_exclusions = [\"#ff\", \"ff\", \"rt\"]\n",
    "stopwords.extend(other_exclusions)\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "# \"\"\" Stemming is a text normalization technique that cuts off the end or beginning of a word by taking into account\n",
    "# a list of common prefixes or suffixes that could be found in that word \"\"\" \"returns words to their root\"\n",
    "\n",
    "\n",
    "def preprocess(text_string):\n",
    "    # \"\"\"\n",
    "    # Accepts a text string and replaces:\n",
    "    # 1) urls with URLHERE\n",
    "    # 2) lots of whitespace with one instance\n",
    "    # 3) mentions with MENTIONHERE\n",
    "\n",
    "    # This allows us to get standardized counts of urls and mentions\n",
    "    # Without caring about specific people mentioned\n",
    "    # \"\"\"\n",
    "    space_pattern = '\\s+'\n",
    "    giant_url_regex = ('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|'\n",
    "        '[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    mention_regex = '@[\\w\\-]+'\n",
    "    parsed_text = re.sub(space_pattern, ' ', text_string)\n",
    "    parsed_text = re.sub(giant_url_regex, '', parsed_text)\n",
    "    parsed_text = re.sub(mention_regex, '', parsed_text)\n",
    "    return parsed_text\n",
    "\n",
    "def tokenize(tweet):\n",
    "    # \"\"\"Removes punctuation & excess whitespace, sets to lowercase,\n",
    "    # and stems tweets. Returns a list of stemmed tokens.\"\"\"\n",
    "    tweet = \" \".join(re.split(\"[^a-zA-Z]*\", tweet.lower())).strip()\n",
    "    tokens = [stemmer.stem(t) for t in tweet.split()]\n",
    "    return tokens\n",
    "\n",
    "def basic_tokenize(tweet):\n",
    "    # \"\"\"Same as tokenize but without the stemming\"\"\"\n",
    "    tweet = \" \".join(re.split(\"[^a-zA-Z.,!?]*\", tweet.lower())).strip()\n",
    "    return tweet.split()\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    tokenizer=tokenize,\n",
    "    preprocessor=preprocess,\n",
    "    ngram_range=(1, 3),\n",
    "    stop_words=stopwords,\n",
    "    use_idf=True,\n",
    "    smooth_idf=False,\n",
    "    norm=None,\n",
    "    decode_error='replace',\n",
    "    max_features=10000,\n",
    "    min_df=5,\n",
    "    max_df=0.75\n",
    "    )\n",
    "\n",
    "#TFIDF VECTORIZER\n",
    "# It can be defined as the calculation of how relevant a word in a series or corpus is to a text.\n",
    "# The meaning increases proportionally to the number of times in the text a word appears\n",
    "# but is compensated by the word frequency in the corpus (data-set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6dJZoj_hg0WA"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "#This just ignores future warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vnIF5oQGg0WB",
    "outputId": "99614039-1121-4c9d-d746-aaf40535ad3e"
   },
   "outputs": [],
   "source": [
    "#Construct tfidf matrix and get relevant scores\n",
    "tfidf = vectorizer.fit_transform(tweets).toarray()\n",
    "vocab = {v:i for i, v in enumerate(vectorizer.get_feature_names_out())}\n",
    "idf_vals = vectorizer.idf_\n",
    "idf_dict = {i:idf_vals[i] for i in vocab.values()} #keys are indices; values are IDF scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CnIQXdezrGTT",
    "outputId": "d84f935d-b458-4df1-8fcd-b96e36b5ab63"
   },
   "outputs": [],
   "source": [
    "#Breaking down last cell\n",
    "# print(vectorizer.fit_transform(tweets))\n",
    "# transforms the tweets into a sparse matrix representation\n",
    "# where each row represents a tweet and each column represents a unique word in the entire collection of tweets.\n",
    "\n",
    "print(vectorizer.fit_transform(tweets).toarray())\n",
    "# This converts the sparse matrix representation into a dense NumPy array.\n",
    "# The resulting tfidf array contains the TF-IDF values for each word in each tweet.\n",
    "\n",
    "# Vocab is a dictionary where keys are unique words in the entire collection of tweets (retrieved using vectorizer.get_feature_names_out())\n",
    "# and values are their corresponding indices in the TF-IDF matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fx7rW3xng0WB"
   },
   "outputs": [],
   "source": [
    "#Get POS tags for tweets and save as a string\n",
    "\n",
    "tweet_tags = []\n",
    "for t in tweets:\n",
    "    tokens = basic_tokenize(preprocess(t))\n",
    "    tags = nltk.pos_tag(tokens)\n",
    "    tag_list = [x[1] for x in tags]\n",
    "    tag_str = \" \".join(tag_list)\n",
    "    tweet_tags.append(tag_str)\n",
    "\n",
    "# The for loop processes a collection of tweets, tokenizes and preprocesses each tweet,\n",
    "# assigns part-of-speech tags to the tokens, and then stores the sequences of part-of-speech tags for each tweet in a list called tweet_tags.\n",
    "#POS tagging, it uses grammer stuff like nouns and verbs adjactives for each word because it helps in understanding the syntax structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fvAMHhs5g0WB"
   },
   "outputs": [],
   "source": [
    "#We can use the TFIDF vectorizer to get a token matrix for the POS tags\n",
    "pos_vectorizer = TfidfVectorizer(\n",
    "    tokenizer=None,\n",
    "    lowercase=False,\n",
    "    preprocessor=None,\n",
    "    ngram_range=(1, 3),\n",
    "    stop_words=None,\n",
    "    use_idf=False,\n",
    "    smooth_idf=False,\n",
    "    norm=None,\n",
    "    decode_error='replace',\n",
    "    max_features=5000,\n",
    "    min_df=5,\n",
    "    max_df=0.75,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4Aus2m2Ng0WC"
   },
   "outputs": [],
   "source": [
    "#Construct POS TF matrix and get vocab dict\n",
    "pos = pos_vectorizer.fit_transform(pd.Series(tweet_tags)).toarray()\n",
    "pos_vocab = {v:i for i, v in enumerate(pos_vectorizer.get_feature_names_out())}\n",
    "\n",
    "#Last two cells are the same as the previous tfidf method, were making a tokens matrix for POS tags this time\n",
    "#then making a tfidf matrix and then getting the vocab dictionary, all for pos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RSj08xWsg0WC"
   },
   "outputs": [],
   "source": [
    "#Now get other features\n",
    "sentiment_analyzer = VS()\n",
    "\n",
    "def count_twitter_objs(text_string):\n",
    "    # \"\"\" function just for statistics\n",
    "    # Accepts a text string and replaces:\n",
    "    # 1) urls with URLHERE\n",
    "    # 2) lots of whitespace with one instance\n",
    "    # 3) mentions with MENTIONHERE\n",
    "    # 4) hashtags with HASHTAGHERE\n",
    "\n",
    "    # This allows us to get standardized counts of urls and mentions\n",
    "    # Without caring about specific people mentioned.\n",
    "\n",
    "    # Returns counts of urls, mentions, and hashtags.\n",
    "    # \"\"\"\n",
    "    space_pattern = '\\s+'\n",
    "    giant_url_regex = ('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|'\n",
    "        '[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    mention_regex = '@[\\w\\-]+'\n",
    "    hashtag_regex = '#[\\w\\-]+'\n",
    "    parsed_text = re.sub(space_pattern, ' ', text_string)\n",
    "    parsed_text = re.sub(giant_url_regex, 'URLHERE', parsed_text)\n",
    "    parsed_text = re.sub(mention_regex, 'MENTIONHERE', parsed_text)\n",
    "    parsed_text = re.sub(hashtag_regex, 'HASHTAGHERE', parsed_text)\n",
    "    return(parsed_text.count('URLHERE'),parsed_text.count('MENTIONHERE'),parsed_text.count('HASHTAGHERE'))\n",
    "\n",
    "def other_features(tweet):\n",
    "    # \"\"\"This function takes a string and returns a list of features.\n",
    "    # These include Sentiment scores, Text and Readability scores,\n",
    "    # as well as Twitter specific features\"\"\"\n",
    "    sentiment = sentiment_analyzer.polarity_scores(tweet)\n",
    "\n",
    "    words = preprocess(tweet) #Get text only\n",
    "\n",
    "    syllables = textstat.syllable_count(words)\n",
    "    num_chars = sum(len(w) for w in words)\n",
    "    num_chars_total = len(tweet)\n",
    "    num_terms = len(tweet.split())\n",
    "    num_words = len(words.split())\n",
    "    avg_syl = round(float((syllables+0.001))/float(num_words+0.001),4)\n",
    "    num_unique_terms = len(set(words.split()))\n",
    "\n",
    "    ###Modified FK grade, where avg words per sentence is just num words/1\n",
    "    FKRA = round(float(0.39 * float(num_words)/1.0) + float(11.8 * avg_syl) - 15.59,1)\n",
    "    ##Modified FRE score, where sentence fixed to 1\n",
    "    FRE = round(206.835 - 1.015*(float(num_words)/1.0) - (84.6*float(avg_syl)),2)\n",
    "\n",
    "    twitter_objs = count_twitter_objs(tweet)\n",
    "    retweet = 0\n",
    "    if \"rt\" in words:\n",
    "        retweet = 1\n",
    "    features = [FKRA, FRE,syllables, avg_syl, num_chars, num_chars_total, num_terms, num_words,\n",
    "                num_unique_terms, sentiment['neg'], sentiment['pos'], sentiment['neu'], sentiment['compound'],\n",
    "                twitter_objs[2], twitter_objs[1],\n",
    "                twitter_objs[0], retweet]\n",
    "    #features = pandas.DataFrame(features)\n",
    "    return features\n",
    "\n",
    "def get_feature_array(tweets):\n",
    "    feats=[]\n",
    "    for t in tweets:\n",
    "        feats.append(other_features(t))\n",
    "    return np.array(feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SiHW6IvlrGTU"
   },
   "outputs": [],
   "source": [
    "# It calculates sentiment scores using a sentiment analyzer. (negative, positive, neutral)\n",
    "# Then,it preprocesses the tweet to extract text-only content and computes various metrics such as syllable count,\n",
    "# character count, word count, average syllables per word, and number of unique terms.\n",
    "\n",
    "# Then it calculates modified versions of Flesch-Kincaid Readability (FKRA) and Flesch Reading Ease (FRE) scores tailored for tweets. \"How easy the data is to read\"\n",
    "# It also identifies Twitter-specific features like mentions, hashtags, and URLs.\n",
    "\n",
    "# Then it constructs a list containing all these features and returns it.\n",
    "# This function is useful for extracting meaningful information from tweets for tasks such as sentiment analysis,\n",
    "# readability assessment, and Twitter-specific analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "npgdeKCFg0WD"
   },
   "outputs": [],
   "source": [
    "other_features_names = [\"FKRA\", \"FRE\",\"num_syllables\", \"avg_syl_per_word\", \"num_chars\", \"num_chars_total\", \\\n",
    "                        \"num_terms\", \"num_words\", \"num_unique_words\", \"vader neg\",\"vader pos\",\"vader neu\", \\\n",
    "                        \"vader compound\", \"num_hashtags\", \"num_mentions\", \"num_urls\", \"is_retweet\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "av-Fopadg0WE"
   },
   "outputs": [],
   "source": [
    "feats = get_feature_array(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uL_IIMCkg0WE"
   },
   "outputs": [],
   "source": [
    "#Now join them all up\n",
    "M = np.concatenate([tfidf,pos,feats],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K4SngMDfg0WE",
    "outputId": "1e07a319-6d8a-4dfb-aefc-ec8302145348"
   },
   "outputs": [],
   "source": [
    "M.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SmSZelbJg0WE"
   },
   "outputs": [],
   "source": [
    "#Finally get a list of variable names\n",
    "variables = ['']*len(vocab)\n",
    "for k,v in vocab.items():\n",
    "    variables[v] = k\n",
    "\n",
    "pos_variables = ['']*len(pos_vocab)\n",
    "for k,v in pos_vocab.items():\n",
    "    pos_variables[v] = k\n",
    "\n",
    "feature_names = variables+pos_variables+other_features_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pbGlz_Mvg0WF"
   },
   "source": [
    "# Running the model\n",
    "\n",
    "The best model was selected using a GridSearch with 5-fold CV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y1UqcI5Mg0WG"
   },
   "outputs": [],
   "source": [
    "X = pd.DataFrame(M)\n",
    "y = df['class'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QP2PCaa2g0WG"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eNRRaAM9g0WH"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fQBp2-mFXycW"
   },
   "outputs": [],
   "source": [
    "####\n",
    "#X_train, X_test, y_train, y_test = train_test_split(df['clean_tweet'], df['class'], test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uBlkZX0Zg0WH",
    "outputId": "5730b45e-9341-47d2-a31d-11e7d9179b05"
   },
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "# from sklearn.svm import SVC\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn.feature_selection import SelectFromModel\n",
    "# from sklearn.svm import SVC\n",
    "# from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn.feature_selection import SelectFromModel\n",
    "# from sklearn.svm import SVC\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt   #for data visualization and graphical plotting\n",
    "# from matplotlib import style      #for styling the plot\n",
    "# style.use(\"ggplot\")\n",
    "\n",
    "# import nltk\n",
    "# from nltk.util import pr\n",
    "# from nltk.tokenize import word_tokenize      #to divide strings into lists of substrings\n",
    "# from nltk.stem import WordNetLemmatizer      #to link words with similar meanings to one word.\n",
    "# from nltk.corpus import stopwords            #to filterout useless data\n",
    "# stopword = set(stopwords.words('english'))\n",
    "\n",
    "# from wordcloud import WordCloud\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# import re\n",
    "# import string\n",
    "# from sklearnex import patch_sklearn\n",
    "# patch_sklearn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JV4ybkI-lcyX",
    "outputId": "6af5370c-6ed2-48e2-e628-e21b074f1a0c"
   },
   "outputs": [],
   "source": [
    "svm = SVC()\n",
    "\n",
    "# Fit the model to the training data\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test data\n",
    "svm_pred = svm.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, svm_pred)\n",
    "print(\"Test accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N8GCShRElcyX",
    "outputId": "1048cf32-d842-4fc5-fb96-28558155d1b2"
   },
   "outputs": [],
   "source": [
    "joblib.dump(svm, 'HateSpeechSVMHyperparameters.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 676
    },
    "id": "MUr3kHukg0WK",
    "outputId": "01c5e479-5307-4bef-c5b0-0d14a640a735"
   },
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, svm_pred)\n",
    "\n",
    "# Define labels for the axes\n",
    "labels = ['Hate Speech', 'Offensive Language', 'No Hate and Offensive']\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", xticklabels=labels, yticklabels=labels)\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(\"Confusion Matrix for SVM\")\n",
    "plt.xticks(ticks=[0, 1, 2], labels=labels, rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QX-jSVafpI8P"
   },
   "outputs": [],
   "source": [
    "#NAME STUDY NOT DEFINED ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EeYd2Er0ZDbL",
    "outputId": "1db29989-4ab6-4512-b047-2d7b2708c5d9"
   },
   "outputs": [],
   "source": [
    "patch_sklearn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gEYJiT63pFCP",
    "outputId": "67cf0619-b386-4b1f-ebc7-01364e16c043"
   },
   "outputs": [],
   "source": [
    "!pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f6SoACGGZRLb"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84,
     "referenced_widgets": [
      "5c334e7f02a044ff9fe0173149e78b41",
      "02a426f247694246bf0b538550bccdb5",
      "c87118fd225349259aed33b03406cb66",
      "2032aae1721348c19c2845e04ee139de",
      "b24f2c2257654f1196e495f37dc371fb",
      "344162839abb44af88239ec7406cd69c",
      "381b6ae3c5084ab38f0a0fe11a473e31",
      "2685e4fb354345faa89fbb4a6190bd04",
      "6de671706bed48dbbbd633ac3bb8ccac",
      "6318560e5f1f4806a75c360b6678e5be",
      "0486da05c67144a4a1b7e271aabc5be3"
     ]
    },
    "id": "-xaB_ldqpCxL",
    "outputId": "2485ee8a-1bfa-466c-ac6d-9c8197a0a107"
   },
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    C = trial.suggest_loguniform('C', 0.1, 100)\n",
    "    kernel = trial.suggest_categorical('kernel', ['linear', 'rbf', 'poly', 'sigmoid'])\n",
    "    gamma = trial.suggest_categorical('gamma', ['scale', 'auto'])\n",
    "\n",
    "    # Create SVM model with suggested hyperparameters\n",
    "    svm = SVC_Intel(C=C, kernel=kernel, gamma=gamma)\n",
    "    svm.fit(X_train, y_train)\n",
    "\n",
    "    # Calculate accuracy score on the test set\n",
    "    accuracy = svm.score(X_test, y_test)\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "# Create an Optuna study\n",
    "study = optuna.create_study(sampler=optuna.samplers.TPESampler(seed=123),\n",
    "                            direction=\"maximize\",\n",
    "                            pruner=optuna.pruners.MedianPruner())\n",
    "\n",
    "# Optimize hyperparameters\n",
    "study.optimize(objective, n_trials=50, show_progress_bar=True)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = study.best_params\n",
    "print(\"Best hyperparameters:\", best_params)\n",
    "\n",
    "# Update SVM model with the best hyperparameters\n",
    "best_svm = SVC_Intel(**best_params)\n",
    "best_svm.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = best_svm.score(X_test, y_test)\n",
    "print(\"Test accuracy with best hyperparameters:\", accuracy)\n",
    "\n",
    "# Save the model\n",
    "joblib.dump(best_svm, 'HateSpeechSVMHyperparameters.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DmLO21IBpVD1"
   },
   "outputs": [],
   "source": [
    "#OLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "syfqDV9UlcyY"
   },
   "outputs": [],
   "source": [
    "# # Hyperparameter tuning\n",
    "# param_grid = {\n",
    "#     'C': [0.1, 1, 10, 100],\n",
    "#     'kernel': ['linear', 'rbf', 'poly', 'sigmoid'],\n",
    "#     'gamma': ['scale', 'auto']\n",
    "# }\n",
    "\n",
    "# study.optimize(objective, n_trials=50, show_progress_bar=True)\n",
    "\n",
    "# grid = GridSearchCV(svm, param_grid, cv=5)\n",
    "# grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rqH47EINlcyY"
   },
   "outputs": [],
   "source": [
    "print(\"Best Cross validation score: {:.2f}\".format(grid.best_score_))\n",
    "print(\"Best parameters: \", grid.best_params_)\n",
    "\n",
    "svm_hyper_pred = grid.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, svm_hyper_pred)\n",
    "print(\"Test accuracy:\", accuracy)\n",
    "\n",
    "#for visualizing confusion matrix\n",
    "style.use('classic')\n",
    "cm = confusion_matrix(y_test, svm_hyper_pred, labels=grid.classes_)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=grid.classes_)\n",
    "disp.plot()\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, svm_hyper_pred)\n",
    "\n",
    "# Define labels for the axes\n",
    "labels = ['Hate Speech', 'Offensive Language', 'No Hate and Offensive']\n",
    "\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", xticklabels=labels, yticklabels=labels)\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(\"Confusion Matrix for Logistic Regression\")\n",
    "plt.xticks(ticks=[0, 1, 2], labels=labels, rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d3N1ERHclcyY"
   },
   "outputs": [],
   "source": [
    "joblib.dump(grid, 'HateSpeechSVM84_6CV2.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QhDT7Js-g0WK"
   },
   "source": [
    "## Evaluating the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fi5nzsTdg0WK"
   },
   "outputs": [],
   "source": [
    "report = classification_report( y_test, y_preds )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RWIHTArHg0WL"
   },
   "source": [
    "***Note: Results in paper are from best model retrained on the entire dataset (see the other notebook). Here the results are reported after using cross-validation and only for the held-out set.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kv50vqF7g0WL"
   },
   "outputs": [],
   "source": [
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o3VKZtgqg0WN"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix = confusion_matrix(y_test,y_preds)\n",
    "matrix_proportions = np.zeros((3,3))\n",
    "for i in range(0,3):\n",
    "    matrix_proportions[i,:] = confusion_matrix[i,:]/float(confusion_matrix[i,:].sum())\n",
    "names=['Hate','Offensive','Neither']\n",
    "confusion_df = pd.DataFrame(matrix_proportions, index=names,columns=names)\n",
    "plt.figure(figsize=(5,5))\n",
    "seaborn.heatmap(confusion_df,annot=True,annot_kws={\"size\": 12},cmap='gist_gray_r',cbar=False, square=True,fmt='.2f')\n",
    "plt.ylabel(r'True categories',fontsize=14)\n",
    "plt.xlabel(r'Predicted categories',fontsize=14)\n",
    "plt.tick_params(labelsize=12)\n",
    "\n",
    "# Basic Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KXHxUt8xg0WO"
   },
   "outputs": [],
   "source": [
    "#True distribution\n",
    "y.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wbMWryZeg0WO"
   },
   "outputs": [],
   "source": [
    "pd.Series(y_preds).hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bIV-y00Qg0WP"
   },
   "outputs": [],
   "source": [
    "#Saving the Model\n",
    "joblib.dump(model, 'HateSpeechSVM81.pkl')\n",
    "# joblib.load(\"HateSpeechTestrun_Model.pkl\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "02a426f247694246bf0b538550bccdb5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_344162839abb44af88239ec7406cd69c",
      "placeholder": "",
      "style": "IPY_MODEL_381b6ae3c5084ab38f0a0fe11a473e31",
      "value": "Besttrial:0.Bestvalue:0.659987:4%"
     }
    },
    "0486da05c67144a4a1b7e271aabc5be3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2032aae1721348c19c2845e04ee139de": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6318560e5f1f4806a75c360b6678e5be",
      "placeholder": "",
      "style": "IPY_MODEL_0486da05c67144a4a1b7e271aabc5be3",
      "value": "2/50[03:33&lt;1:22:45,103.45s/it]"
     }
    },
    "2685e4fb354345faa89fbb4a6190bd04": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "344162839abb44af88239ec7406cd69c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "381b6ae3c5084ab38f0a0fe11a473e31": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5c334e7f02a044ff9fe0173149e78b41": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_02a426f247694246bf0b538550bccdb5",
       "IPY_MODEL_c87118fd225349259aed33b03406cb66",
       "IPY_MODEL_2032aae1721348c19c2845e04ee139de"
      ],
      "layout": "IPY_MODEL_b24f2c2257654f1196e495f37dc371fb"
     }
    },
    "6318560e5f1f4806a75c360b6678e5be": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6de671706bed48dbbbd633ac3bb8ccac": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "b24f2c2257654f1196e495f37dc371fb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c87118fd225349259aed33b03406cb66": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2685e4fb354345faa89fbb4a6190bd04",
      "max": 50,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_6de671706bed48dbbbd633ac3bb8ccac",
      "value": 2
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
